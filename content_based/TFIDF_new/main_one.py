import os
import argparse
import numpy as np
import random
import json
from openai import OpenAI
from dotenv import load_dotenv
from search.db_search import DatabaseHandler
from search.dense_retriever import DenseRetriever
from search.dataset_json import goalDatasetjson
from recomm.recomm import Recommendation

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
DB_PWD = os.getenv('DB_PWD')
OPENAI_API_KEY = os.getenv('PF_OPEN_AI_KEY')

# ğŸ”¹ ì¶”ì²œ ê²°ê³¼ ì¼ê´€ì„± ìœ ì§€ (TF-IDF ê¸°ë°˜ ì¶”ì²œ ì‹œ)
def set_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)

def get_args():
    parser = argparse.ArgumentParser(description="ê°•ì˜ ì¶”ì²œ ì‹œìŠ¤í…œ(TF-IDF ê¸°ë°˜)")
    parser.add_argument("--query", type=str, required=True, help="ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬")
    parser.add_argument("--expanded_query", type=str, default="", help="í™•ì¥ëœ í…ìŠ¤íŠ¸ (ì„ íƒ ì‚¬í•­)")
    parser.add_argument("--dept", type=str, choices=["yes", "no"], required=True, default="no", help="í•™ê³¼ ì§€ì •í• ì§€ ì—¬ë¶€ (yes / no)")
    parser.add_argument("--departments", type=str, default="", help="í•™ê³¼ ë¦¬ìŠ¤íŠ¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)")
    
    parser.add_argument("--department_path", type=str, default="/root/ai_curri/content_based/TFIDF_new/data/depart_info.json", help="í•™ê³¼ ì†Œê°œê¸€ ì„ë² ë”© í•  í…ìŠ¤íŠ¸")
    parser.add_argument("--batch_size", type=int, default=10, help="ê²€ìƒ‰ì— ì‚¬ìš©ë˜ëŠ” batch size")

    return parser.parse_args()

def main():
    args = get_args()

    set_seed()
    
    db_config = {
        "host": "210.117.181.113",
        "port": 3311,
        "user": "root",
        "password": DB_PWD,
        "database": "nll_third",
        "charset": "utf8mb4"
    }
    
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    db_handler = DatabaseHandler(
        host=db_config["host"],
        port=db_config["port"],
        user=db_config["user"],
        password=db_config["password"],
        database=db_config["database"],
        charset=db_config["charset"]
    )
    
    db_handler.connect()
    print("Fetching department data...")
    
    # ğŸ”¹ í•™ê³¼ ë²¡í„° ì €ì¥ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
    if not os.path.exists("data/dept_vectorizer.pkl"):
        dept_info = goalDatasetjson(json.load(open(args.department_path)))
        retriever = DenseRetriever(client, args, dept_info)
        retriever.doc_embedding()
    else:
        retriever = DenseRetriever(client, args)
        retriever.doc_embedding()
    
    recomm = Recommendation(db_handler)

    # ğŸ”¹ êµê³¼ëª© ë²¡í„° ì €ì¥ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
    if not os.path.exists("data/tfidf_vectors.npz") or not os.path.exists("data/course_vectorizer.pkl"):
        recomm.preprocess_and_save()
        
    # ğŸ”¹ ì¿¼ë¦¬ ì„¤ì •
    query = args.query
    expanded_query = args.expanded_query if args.expanded_query else query  # í™•ì¥ëœ ì¿¼ë¦¬ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
    query_info = query if args.expanded_query == "" else expanded_query

    # ğŸ”¹ í•™ê³¼ ì •ë³´ ì„¤ì •
    if args.dept == "no":
        query_embedding = retriever.query_embedding(query_info)
        department_list = retriever.retrieve(query_embedding)
        department_list = [dept["department_name"] for dept in department_list]
        print(f"í•™ê³¼ ê²€ìƒ‰ ê²°ê³¼ ::: {department_list} \n\n")
    else:
        department_list = [dept.strip() for dept in args.departments.split(",")]

    # ğŸ”¹ ê°•ì˜ ì¶”ì²œ ì‹¤í–‰
    recommendations = recomm.recommend_classes_dept(query_info, department_list)

    # ğŸ”¹ JSON ì €ì¥ í˜•ì‹ êµ¬ì„±
    result_data = {
        "meta_info": {
            "user_query": query,
            "expanded_query": expanded_query if args.expanded_query else None,
            "given_departments": department_list if args.dept == "yes" else None,
            "selected_departments": department_list if args.dept == "no" else None
        },
        "recommendations": recommendations  # (ìµœì¢… ê²°ê³¼)
    }

    # ğŸ”¹ ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    save_folder_dept = "original" if args.dept == "no" else "given_dept"
    output_dir = f'/root/ai_curri/content_based/TFIDF_new/output'
    os.makedirs(output_dir, exist_ok=True)
    json_file = f'{output_dir}/ìŠ¤ë§ˆíŠ¸ê¸€ë¡œë²Œë¬´ì—­_recommendations_{("yes" if args.expanded_query else "no")}.json'
    
    # ğŸ”¹ JSON íŒŒì¼ë¡œ ì €ì¥ (í•™ê³¼ ì •ë³´ í¬í•¨)
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(result_data, f, ensure_ascii=False, indent=4)

    print(f"âœ… ì¶”ì²œ ê²°ê³¼ê°€ {json_file} íŒŒì¼ë¡œ ì €ì¥ë¨!")

    # ğŸ”¹ ì¶”ì²œ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ”¹ ì¶”ì²œ ê°•ì˜ ëª©ë¡:")
    for rec in recommendations:
        print(f"ğŸ“Œ [{rec['student_grade']}í•™ë…„ {rec['semester']}í•™ê¸°] {rec['name']} "
              f"(ìœ ì‚¬ë„: {rec['similarity']:.2f}) - {rec['department_name']}")

if __name__ == "__main__":
    main()