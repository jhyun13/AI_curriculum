import os
import argparse
import numpy as np
import random
import json
from openai import OpenAI
from dotenv import load_dotenv
from search.db_search import DatabaseHandler
from search.dense_retriever import DenseRetriever
from search.dataset_json import goalDatasetjson
from recomm.recomm import Recommendation

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
DB_PWD = os.getenv('DB_PWD')
OPENAI_API_KEY = os.getenv('PF_OPEN_AI_KEY')

# ğŸ”¹ ì¶”ì²œ ê²°ê³¼ ì¼ê´€ì„± ìœ ì§€ (TF-IDF ê¸°ë°˜ ì¶”ì²œ ì‹œ)
def set_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)

def get_args():
    parser = argparse.ArgumentParser(description="ê°•ì˜ ì¶”ì²œ ì‹œìŠ¤í…œ(TF-IDF ê¸°ë°˜)")
    parser.add_argument("--query_path", type=str, required=True, help="ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--expand", type=str, choices=["yes", "no"], required=True, default="no", help="í…ìŠ¤íŠ¸ë¥¼ í™•ì¥í• ì§€ ì—¬ë¶€ (yes / no)")
    parser.add_argument("--dept", type=str, choices=["yes", "no", "auto"], required=True, default="no", help="í•™ê³¼ ì§€ì •í• ì§€ ì—¬ë¶€ (yes / no)")
    # parser.add_argument("--save_path", type=str, required=True, help="ê²°ê³¼ ì €ì¥í•  json íŒŒì¼ ê²½ë¡œ")
    
    parser.add_argument("--department_path", type=str, default="/root/ai_curri/content_based/TFIDF_new/data/depart_info.json", help="í•™ê³¼ ì†Œê°œê¸€ ì„ë² ë”© í•  í…ìŠ¤íŠ¸")
    parser.add_argument("--batch_size", type=int, default=10, help="ê²€ìƒ‰ì— ì‚¬ìš©ë˜ëŠ” batch size")

    return parser.parse_args()

def main():
    args = get_args()

    set_seed()
    
    db_config = {
        "host": "210.117.181.113",
        "port": 3311,
        "user": "root",
        "password": DB_PWD,
        "database": "nll_third",
        "charset": "utf8mb4"
    }
    
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    db_handler = DatabaseHandler(
        host=db_config["host"],
        port=db_config["port"],
        user=db_config["user"],
        password=db_config["password"],
        database=db_config["database"],
        charset=db_config["charset"]
    )
    
    db_handler.connect()
    print("Fetching department data...")
    
        
    # ğŸ”¹ í•™ê³¼ ë²¡í„° ì €ì¥ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
    if not os.path.exists("data/dept_vectorizer.pkl"):
        dept_info = goalDatasetjson(json.load(open(args.department_path)))
        retriever = DenseRetriever(client, args, dept_info)
        retriever.doc_embedding()
    else:
        retriever = DenseRetriever(client, args)
        retriever.doc_embedding()
        
    with open(args.query_path, "r", encoding="utf-8") as file:
        input_query = json.load(file)

    recomm = Recommendation(db_handler)

    # ğŸ”¹ êµê³¼ëª© ë²¡í„° ì €ì¥ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
    if not os.path.exists("data/tfidf_vectors.npz") or not os.path.exists("data/course_vectorizer.pkl"):
        recomm.preprocess_and_save()
        
    # ê° ì „ê³µë³„ë¡œ queryì™€ expanded_query ì²˜ë¦¬
    for major, details in input_query.items():
        print(f"Processing Major: {major}")
        departments = details.get("departments", "")
        
        for idx, query_data in enumerate(details.get("query_list", [])):
            query = query_data.get("query", "")
            expanded_query = query_data.get("expanded_query", "")
            
            # ì¿¼ë¦¬ í™•ì¥ ì˜µì…˜ ì„ íƒ
            query_info = query if args.expand == "no" else expanded_query
            query_embbeding = retriever.query_embedding(query_info)
            
            # í•™ê³¼ ì£¼ì–´ì§€ëŠ” ì˜µì…˜ ì„ íƒ
            if args.dept == "auto":
                department_list = retriever.retrieve(query_embbeding)
                department_list = [dept["department_name"] for dept in department_list]
                print(f"í•™ê³¼ ê²€ìƒ‰ ê²°ê³¼ ::: {department_list} \n\n")
                recommendations = recomm.recommend_classes_dept(query if args.expand == "no" else expanded_query, department_list)
                
            elif args.dept == "yes":
                department_list = [dept.strip() for dept in departments.split(",")]
                recommendations = recomm.recommend_classes_dept(query if args.expand == "no" else expanded_query, department_list)
                
            else:
                recommendations = recomm.recommend_classes(query if args.expand == "no" else expanded_query)


            # ğŸ”¹ JSON ì €ì¥ í˜•ì‹ êµ¬ì„± (ë©”íƒ€ì •ë³´ í¬í•¨)
            result_data = {
                "meta_info": {
                    "user_query": query,
                    "expanded_query": expanded_query if args.expand == "yes" else None,
                    "given_departments": department_list if args.dept == "yes" else None,
                    "selected_departments": department_list if args.dept == "auto" else None
                },
                "recommendations": recommendations # (final result)
            }
            
            if args.dept == "auto":
                save_folder_dept = "original"
            elif args.dept == "yes":
                save_folder_dept = "given_dept"
            else:
                save_folder_dept = "no_dept"
            
            output_dir = f'/root/ai_curri/content_based/TFIDF_new/output/new/{save_folder_dept}/query_exp_{args.expand.upper()}'
            os.makedirs(output_dir, exist_ok=True)
            json_file = f'{output_dir}/{major}({idx+1})_recommendations_{args.expand}.json'
                
            # ğŸ”¹ JSON íŒŒì¼ë¡œ ì €ì¥ (í•™ê³¼ ì •ë³´ í¬í•¨)
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(result_data, f, ensure_ascii=False, indent=4)

            print(f"âœ… ì¶”ì²œ ê²°ê³¼ê°€ {json_file} íŒŒì¼ë¡œ ì €ì¥ë¨!")
    

            # ğŸ”¹ ì¶”ì²œ ê²°ê³¼ ì¶œë ¥
            print("\nğŸ”¹ ì¶”ì²œ ê°•ì˜ ëª©ë¡:")
            for rec in recommendations:
                print(f"ğŸ“Œ [{rec['student_grade']}í•™ë…„ {rec['semester']}í•™ê¸°] {rec['name']} "
                    f"(ìœ ì‚¬ë„: {rec['similarity']:.2f}) - {rec['department_name']}")

if __name__ == "__main__":
    main()